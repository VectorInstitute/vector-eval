{"basic_rag": {"cohere-command-r-plus": {"task_cfg": {"task_name": "wikieval", "dataset_path": "explodinggradients/WikiEval", "dataset_name": null, "dataset_kwargs": {"trust_remote_code": true}, "validation_split": "train", "data_instance_map": {"query": "question", "gt_answer": "answer", "gt_context": "context_v1"}, "docs_path": "veval/tasks/wikieval/doc_store", "metric_list": [{"metric": "relevance_query_answer", "args": ["query", "context", "answer"]}, {"metric": "groundedness_context_answer", "args": ["query", "context", "answer"]}, {"metric": "relevance_query_context", "args": ["query", "context"]}, {"metric": "correctness_answer", "args": ["query", "answer", "gt_answer"]}]}, "system_cfg": {"sys_name": "basic_rag", "llm_name": "cohere-command-r-plus", "llm_gen_args": {"temperature": 0.8, "max_tokens": 128}, "prompt_template": "\nAnswer the question using information from the provided references.\n\n### Question\n{query_str}\n\n### References \n{context_str}\n\n### Answer\n", "chunk_size": 256, "chunk_overlap": 0, "embed_model_name": "openai-text-embedding-3-small", "similarity_top_k": 5, "response_mode": "compact"}, "result": {"num_samples": 50, "scores": {"vector_retriever": {"relevance_query_answer": 0.11323920230146793, "groundedness_context_answer": 0.26666666666666666, "relevance_query_context": 0.0342147920788673, "correctness_answer": 0.300783682823757}}}, "metadata": {"limit": null, "log_file": "2024-05-13_12:35:31.json"}}, "openai-gpt-3.5-turbo": {"task_cfg": {"task_name": "wikieval", "dataset_path": "explodinggradients/WikiEval", "dataset_name": null, "dataset_kwargs": {"trust_remote_code": true}, "validation_split": "train", "data_instance_map": {"query": "question", "gt_answer": "answer", "gt_context": "context_v1"}, "docs_path": "veval/tasks/wikieval/doc_store", "metric_list": [{"metric": "relevance_query_answer", "args": ["query", "context", "answer"]}, {"metric": "groundedness_context_answer", "args": ["query", "context", "answer"]}, {"metric": "relevance_query_context", "args": ["query", "context"]}, {"metric": "correctness_answer", "args": ["query", "answer", "gt_answer"]}]}, "system_cfg": {"sys_name": "basic_rag", "llm_name": "openai-gpt-3.5-turbo", "llm_gen_args": {"temperature": 0.8, "max_tokens": 128}, "prompt_template": "\nAnswer the question using information from the provided references.\n\n### Question\n{query_str}\n\n### References \n{context_str}\n\n### Answer\n", "chunk_size": 256, "chunk_overlap": 0, "embed_model_name": "openai-text-embedding-3-small", "similarity_top_k": 5, "response_mode": "compact"}, "result": {"num_samples": 50, "scores": {"vector_retriever": {"relevance_query_answer": 0.43499353785271916, "groundedness_context_answer": 0.29492753623188406, "relevance_query_context": 0.0342147920788673, "correctness_answer": 0.3187418767523303}}}, "metadata": {"limit": null, "log_file": "2024-05-13_15:26:02.json"}}, "llama3-8b": {"task_cfg": {"task_name": "wikieval", "dataset_path": "explodinggradients/WikiEval", "dataset_name": null, "dataset_kwargs": {"trust_remote_code": true}, "validation_split": "train", "data_instance_map": {"query": "question", "gt_answer": "answer", "gt_context": "context_v1"}, "docs_path": "veval/tasks/wikieval/doc_store", "metric_list": [{"metric": "relevance_query_answer", "args": ["query", "context", "answer"]}, {"metric": "groundedness_context_answer", "args": ["query", "context", "answer"]}, {"metric": "relevance_query_context", "args": ["query", "context"]}, {"metric": "correctness_answer", "args": ["query", "answer", "gt_answer"]}]}, "system_cfg": {"sys_name": "basic_rag", "llm_name": "llama3-8b", "llm_gen_args": {"temperature": 0.8, "max_tokens": 128}, "prompt_template": "\nAnswer the question using information from the provided references.\n\n### Question\n{query_str}\n\n### References \n{context_str}\n\n### Answer\n", "chunk_size": 256, "chunk_overlap": 0, "embed_model_name": "openai-text-embedding-3-small", "similarity_top_k": 5, "response_mode": "compact"}, "result": {"num_samples": 50, "scores": {"vector_retriever": {"relevance_query_answer": 0.8846964086793432, "groundedness_context_answer": 0.22413793103448276, "relevance_query_context": 0.0342147920788673, "correctness_answer": 0.33259012548909594}}}, "metadata": {"limit": null, "log_file": "2024-05-14_12:42:54.json"}}}, "rerank_rag": {"cohere-command-r-plus": {"task_cfg": {"task_name": "wikieval", "dataset_path": "explodinggradients/WikiEval", "dataset_name": null, "dataset_kwargs": {"trust_remote_code": true}, "validation_split": "train", "data_instance_map": {"query": "question", "gt_answer": "answer", "gt_context": "context_v1"}, "docs_path": "veval/tasks/wikieval/doc_store", "metric_list": [{"metric": "relevance_query_answer", "args": ["query", "context", "answer"]}, {"metric": "groundedness_context_answer", "args": ["query", "context", "answer"]}, {"metric": "relevance_query_context", "args": ["query", "context"]}, {"metric": "correctness_answer", "args": ["query", "answer", "gt_answer"]}]}, "system_cfg": {"sys_name": "rerank_rag", "llm_name": "cohere-command-r-plus", "llm_gen_args": {"temperature": 0.8, "max_tokens": 128}, "prompt_template": "\nAnswer the question using information from the provided references.\n\n### Question\n{query_str}\n\n### References \n{context_str}\n\n### Answer\n", "chunk_size": 256, "chunk_overlap": 0, "embed_model_name": "openai-text-embedding-3-small", "similarity_top_k": 5, "response_mode": "compact", "rerank_llm_name": "openai-gpt-3.5-turbo", "rerank_llm_gen_args": {"temperature": 0}, "rerank_top_k": 3}, "result": {"num_samples": 50, "scores": {"vector_retriever": {"relevance_query_answer": 0.0930931566053841, "groundedness_context_answer": 0.446875, "relevance_query_context": 0.02665390818447879, "correctness_answer": 0.22309582838156888}, "reranker": {"relevance_query_answer": 0.10859108621466605, "groundedness_context_answer": 0.4666666666666666, "relevance_query_context": 0.06377904405535985, "correctness_answer": 0.23827913826860656}}}, "metadata": {"limit": null, "log_file": "2024-05-13_15:00:15.json"}}, "openai-gpt-3.5-turbo": {"task_cfg": {"task_name": "wikieval", "dataset_path": "explodinggradients/WikiEval", "dataset_name": null, "dataset_kwargs": {"trust_remote_code": true}, "validation_split": "train", "data_instance_map": {"query": "question", "gt_answer": "answer", "gt_context": "context_v1"}, "docs_path": "veval/tasks/wikieval/doc_store", "metric_list": [{"metric": "relevance_query_answer", "args": ["query", "context", "answer"]}, {"metric": "groundedness_context_answer", "args": ["query", "context", "answer"]}, {"metric": "relevance_query_context", "args": ["query", "context"]}, {"metric": "correctness_answer", "args": ["query", "answer", "gt_answer"]}]}, "system_cfg": {"sys_name": "rerank_rag", "llm_name": "openai-gpt-3.5-turbo", "llm_gen_args": {"temperature": 0.8, "max_tokens": 128}, "prompt_template": "\nAnswer the question using information from the provided references.\n\n### Question\n{query_str}\n\n### References \n{context_str}\n\n### Answer\n", "chunk_size": 256, "chunk_overlap": 0, "embed_model_name": "openai-text-embedding-3-small", "similarity_top_k": 5, "response_mode": "compact", "rerank_llm_name": "openai-gpt-3.5-turbo", "rerank_llm_gen_args": {"temperature": 0}, "rerank_top_k": 3}, "result": {"num_samples": 50, "scores": {"vector_retriever": {"relevance_query_answer": 0.17076868533788273, "groundedness_context_answer": 0.42180451127819546, "relevance_query_context": 0.026299153368408264, "correctness_answer": 0.25985552103085974}, "reranker": {"relevance_query_answer": 0.17178286464436696, "groundedness_context_answer": 0.625, "relevance_query_context": 0.07047723475355054, "correctness_answer": 0.26791085776954854}}}, "metadata": {"limit": null, "log_file": "2024-05-13_15:26:02.json"}}, "llama3-8b": {"task_cfg": {"task_name": "wikieval", "dataset_path": "explodinggradients/WikiEval", "dataset_name": null, "dataset_kwargs": {"trust_remote_code": true}, "validation_split": "train", "data_instance_map": {"query": "question", "gt_answer": "answer", "gt_context": "context_v1"}, "docs_path": "veval/tasks/wikieval/doc_store", "metric_list": [{"metric": "relevance_query_answer", "args": ["query", "context", "answer"]}, {"metric": "groundedness_context_answer", "args": ["query", "context", "answer"]}, {"metric": "relevance_query_context", "args": ["query", "context"]}, {"metric": "correctness_answer", "args": ["query", "answer", "gt_answer"]}]}, "system_cfg": {"sys_name": "rerank_rag", "llm_name": "llama3-8b", "llm_gen_args": {"temperature": 0.8, "max_tokens": 128}, "prompt_template": "\nAnswer the question using information from the provided references.\n\n### Question\n{query_str}\n\n### References \n{context_str}\n\n### Answer\n", "chunk_size": 256, "chunk_overlap": 0, "embed_model_name": "openai-text-embedding-3-small", "similarity_top_k": 5, "response_mode": "compact", "rerank_llm_name": "openai-gpt-3.5-turbo", "rerank_llm_gen_args": {"temperature": 0}, "rerank_top_k": 3}, "result": {"num_samples": 50, "scores": {"vector_retriever": {"relevance_query_answer": 0.3441087882054728, "groundedness_context_answer": 0.5750000000000001, "relevance_query_context": 0.02616964155505087, "correctness_answer": 0.271573553597128}, "reranker": {"relevance_query_answer": 0.3402991631850068, "groundedness_context_answer": 0.3153846153846154, "relevance_query_context": 0.05798864001495581, "correctness_answer": 0.2762232499178374}}}, "metadata": {"limit": null, "log_file": "2024-05-14_12:42:54.json"}}}}